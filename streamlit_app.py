import streamlit as st
import pandas as pd
from github import Github  # veillez Ã  avoir "PyGithub" dans requirements.txt
from serpapi import GoogleSearch
import requests

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 1. Gestion des secrets (.streamlit/secrets.toml)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# CrÃ©ez un fichier .streamlit/secrets.toml contenant :
# serpapi_key = "VOTRE_CLE_SERPAPI"
SERPAPI_KEY = st.secrets["serpapi_key"]

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 2. Lecture de queries.txt depuis GitHub (public)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
GITHUB_REPO = "thibault60/scraper-SERP"  # laissez vide pour forker plus tard
QUERY_FILE = "queries.txt"

github = Github()  # accÃ¨s en lecture seule aux dÃ©pÃ´ts publics
repo = github.get_repo(GITHUB_REPO)
contents = repo.get_contents(QUERY_FILE)
raw_txt = requests.get(contents.download_url).text

# Chaque ligne de queries.txt est considÃ©rÃ©e comme une requÃªte
queries = [line.strip() for line in raw_txt.splitlines() if line.strip()]

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 3. Fonction dâ€™extraction des PAA (People Also Ask)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def get_paa(query: str) -> list[dict]:
    """Interroge SerpApi et renvoie la liste des blocs PAA."""
    params = {
        "q": query,
        "api_key": SERPAPI_KEY,
        "hl": "fr",
        "gl": "fr",
        "num": 10,
    }
    search = GoogleSearch(params)
    results = search.get_dict()
    return results.get("related_questions", [])  # ClÃ© JSON pour les PAA

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 4. Mise en cache pour limiter les appels API
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

@st.cache_data(show_spinner="ğŸ”„ RÃ©cupÃ©ration des PAA â€¦")
def fetch_paa(queries_list: list[str]) -> pd.DataFrame:
    """Boucle sur chaque requÃªte et gÃ©nÃ¨re un DataFrame Ã  plat."""
    rows = []
    for q in queries_list:
        paa_items = get_paa(q)
        if paa_items:
            for item in paa_items:
                rows.append(
                    {
                        "RequÃªte": q,
                        "Question PAA": item.get("question", "â€”"),
                        "RÃ©ponse": item.get("snippet")
                        or item.get("answer")
                        or "â€”",
                        "Source": item.get("link", ""),
                    }
                )
        else:
            rows.append(
                {
                    "RequÃªte": q,
                    "Question PAA": "â€”",
                    "RÃ©ponse": "â€”",
                    "Source": "",
                }
            )
    return pd.DataFrame(rows)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 5. Interface Streamlit
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

st.set_page_config(page_title="Scraper PAA SERP", layout="wide")
st.title("ğŸ¤– Extraction des PAA Google â€“ scraper-SERP")
st.markdown(
    "Cette application lit `queries.txt` depuis GitHub, interroge SerpApi et affiche les blocs *People Also Ask* (PAA)."
)

if st.button("ğŸ•¹ï¸ Extraire les PAA"):
    df_paa = fetch_paa(queries)
    st.dataframe(df_paa, use_container_width=True)

    # TÃ©lÃ©chargement CSV
    csv = df_paa.to_csv(index=False).encode("utf-8")
    st.download_button(
        label="ğŸ’¾ TÃ©lÃ©charger le CSV",
        data=csv,
        file_name="paa_results.csv",
        mime="text/csv",
    )

    # Affichage dÃ©taillÃ© (groupÃ© par requÃªte)
    for query in df_paa["RequÃªte"].unique():
        subset = df_paa[df_paa["RequÃªte"] == query]
        with st.expander(f"ğŸ” {query} â€” {len(subset)} questions"):
            for _, r in subset.iterrows():
                st.markdown(f"**Q:** {r['Question PAA']}")
                if r["RÃ©ponse"] != "â€”":
                    st.markdown(r["RÃ©ponse"])
                if r["Source"]:
                    st.caption(r["Source"])
                st.markdown("---")
